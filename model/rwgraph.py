import torch
import torch.nn as nn
import torch.nn.functional as F

from torch.autograd import Variable
from torch.nn.parameter import Parameter


class RW_NN(nn.Module):
    def __init__(self, input_dim, batch_size, clip_len, max_step, hidden_graphs, size_hidden_graphs,
                 hidden_dim, penultimate_dim, normalize, n_classes, dropout, device):
        super(RW_NN, self).__init__()
        self.batch_size = batch_size
        self.max_step = max_step
        self.clip_len = clip_len
        self.hidden_graphs = hidden_graphs
        self.size_hidden_graphs = size_hidden_graphs
        self.normalize = normalize
        self.device = device
        self.adj_hidden = Parameter(
            torch.FloatTensor(hidden_graphs, (size_hidden_graphs * (size_hidden_graphs - 1)) // 2))
        self.features_hidden = Parameter(torch.FloatTensor(hidden_graphs, size_hidden_graphs, hidden_dim))
        self.fc = torch.nn.Linear(input_dim, hidden_dim)
        self.bn = nn.BatchNorm1d(hidden_graphs * max_step)
        # self.bn = nn.BatchNorm1d(hidden_graphs)
        self.fc1 = torch.nn.Linear(hidden_graphs * max_step, penultimate_dim)
        self.fc2 = torch.nn.Linear(penultimate_dim, n_classes)
        self.dropout = nn.Dropout(p=dropout)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.init_weights()

        self.hidden_graphs = hidden_graphs
        self.max_step = max_step

    def init_weights(self):
        self.adj_hidden.data.uniform_(-1, 1)
        self.features_hidden.data.uniform_(0, 1)

    def forward(self, adj, features, graph_indicator):
        unique, counts = torch.unique(graph_indicator, return_counts=True)
        n_graphs = unique.size(0)
        n_nodes = features.size(0)

        if self.normalize:
            norm = counts.unsqueeze(1).repeat(1, self.hidden_graphs)

        adj_hidden_norm = torch.zeros(self.hidden_graphs, self.size_hidden_graphs, self.size_hidden_graphs).to(
            self.device)
        idx = torch.triu_indices(self.size_hidden_graphs, self.size_hidden_graphs, 1)
        adj_hidden_norm[:, idx[0], idx[1]] = self.relu(self.adj_hidden)
        adj_hidden_norm = adj_hidden_norm + torch.transpose(adj_hidden_norm, 1, 2)
        x = self.sigmoid(self.fc(features))
        z = self.features_hidden
        zx = torch.einsum("abc,dc->abd", (z, x))

        out = list()
        for i in range(self.max_step):
            if i == 0:
                eye = torch.eye(self.size_hidden_graphs, device=self.device)
                eye = eye.repeat(self.hidden_graphs, 1, 1)
                o = torch.einsum("abc,acd->abd", (eye, z))
                t = torch.einsum("abc,dc->abd", (o, x))
            else:
                x = torch.spmm(adj, x)
                z = torch.einsum("abc,acd->abd", (adj_hidden_norm, z))
                t = torch.einsum("abc,dc->abd", (z, x))
            t = self.dropout(t)
            t = torch.mul(zx, t)
            t = torch.zeros(t.size(0), t.size(1), n_graphs, device=self.device).index_add_(2, graph_indicator, t)
            t = torch.sum(t, dim=1)
            t = torch.transpose(t, 0, 1)
            if self.normalize:
                t /= norm
            # out.append(t.unsqueeze(-1))
            ##################################
            t = t.view(self.batch_size, self.clip_len, t.shape[1])
            t = torch.mean(t, 1)
            out.append(t)

        out = torch.cat(out, dim=1)
        # out = out.view(self.batch_size, self.clip_len, out.shape[1])
        # out = torch.mean(out, dim=1)
        out = self.bn(out)
        out = self.relu(self.fc1(out))
        out = self.dropout(out)
        out = self.fc2(out)
        return F.log_softmax(out, dim=1)
        # out = torch.cat(out, dim=1)
        # # out = self.bn(out)
        # # out = self.fc1(out)
        # out = self.relu(out)
        # # out = self.dropout(out)
        # # out = self.fc2(out)
        # # return F.log_softmax(out, dim=1)
        # # return out
        #
        # return out.view(out.shape[0], self.hidden_graphs, self.max_step)



if __name__ == '__main__':
    r_list = list()
    for i in range(48):
        aa = torch.Tensor(16,16)
        r_list.append(aa)

    r = torch.stack(r_list, dim=0)
    print(r_list.shape)